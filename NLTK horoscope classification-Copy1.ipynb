{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ebryx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ebryx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ebryx/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "# from xml.dom import minidom\n",
    "from lxml import etree\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to read All csv ::  0.03855180740356445  seconds \n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "path='./dataset/blogs_train/'\n",
    "all_files = glob.glob(path + \"/*.xml\")\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "  # print(\"file name \", filename)\n",
    "  li.append(filename)\n",
    "    \n",
    "\n",
    "t2=time.time()\n",
    "print(\"time taken to read All csv :: \", t2-t1, \" seconds \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file_names(li):\n",
    "  # names_list=[]\n",
    "  # for i in range(len(li)):\n",
    "  name_list=li.split('/')[-1]\n",
    "  return name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_attributes_from_name(name):\n",
    "  # data_object=[]\n",
    "  # for data in range(len(names_list)):\n",
    "  #   dummy_var=names_list[data].split('.')\n",
    "  #   dummy_object={'id':dummy_var[0], 'gender':dummy_var[1],\n",
    "  #                 'age':dummy_var[2], 'category':dummy_var[3],\n",
    "  #                 'horoscope':dummy_var[4]\n",
    "  #                 }\n",
    "  #   data_object.append(dummy_object)\n",
    "  dummy_var=name.split('.')\n",
    "  data_object={'id':dummy_var[0], 'gender':dummy_var[1],\n",
    "                  'age':dummy_var[2], 'category':dummy_var[3],\n",
    "                  'horoscope':dummy_var[4]\n",
    "                }\n",
    "  \n",
    "  return data_object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = etree.XMLParser(encoding='ISO-8859-1', recover = True, huge_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "def clean_my_string(mystring):\n",
    "#     mystring=mystring.replace('..', \"\")\n",
    "\n",
    "#     mystring=sent_tokenize(mystring)    \n",
    "#     words = word_tokenize(mystring)\n",
    "    \n",
    "#     ps = PorterStemmer()\n",
    "#     sentence=''\n",
    "#     for w in words:\n",
    "#         rootWord=ps.stem(w)\n",
    "#         sentence+= \" \" + rootWord\n",
    "    clean_string=[]\n",
    "    \n",
    "    for i in range(len(mystring)): \n",
    "    \n",
    "    \n",
    "\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        sentence=''\n",
    "        tokenization = nltk.word_tokenize(mystring[i])\n",
    "        for w in tokenization:\n",
    "            rootWord=wordnet_lemmatizer.lemmatize(w)\n",
    "            sentence+= \" \" + rootWord\n",
    "        \n",
    "        clean_string.append(sentence)\n",
    "    mystring=clean_string\n",
    "    \n",
    "#     mystring=remove_stop_words(sentence)\n",
    "    return mystring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "def remove_stop_words(mystring):  \n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    filtered_string=[]\n",
    "    \n",
    "    for i in range(len(mystring)):\n",
    "        word_tokens = word_tokenize(mystring[i]) \n",
    "\n",
    "#     filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "        filtered_sentence = [] \n",
    "\n",
    "        for w in word_tokens: \n",
    "            if w not in stop_words: \n",
    "                filtered_sentence.append(w) \n",
    "                \n",
    "        filtered_string.append(filtered_sentence)\n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(li):\n",
    "  all_objects=[]\n",
    "  for i in range(len(li)):\n",
    "    # print(\"i will run \", i+1,\" times\")\n",
    "    name_of_file=clean_file_names(li[i])\n",
    "    # print(name_of_file)\n",
    "    data_object=get_attributes_from_name(name_of_file)\n",
    "    # print(data_object)\n",
    "\n",
    "    post_text='  '\n",
    "    tree = etree.parse(li[i], parser=parser)\n",
    "    root = tree.getroot()\n",
    "    for element in root.iter():\n",
    "        if element.tag == 'post':\n",
    "            element_text = element.text\n",
    "            post_text = post_text + element_text\n",
    "    mystring = post_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "    mystring = mystring.replace('   ', '').replace('\\\\','')\n",
    "   \n",
    "\n",
    "#     mystring=clean_my_string(mystring)\n",
    "   \n",
    "    # print(mystring)\n",
    "    # name_of_file=clean_file_names(li[i])\n",
    "    # data_object=get_attributes_from_name(name_of_file)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    data_object['posts']=mystring   \n",
    "    all_objects.append(data_object)\n",
    "\n",
    "  return all_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "data=read_xml(li[0:])\n",
    "t2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to read ::  0.12011540651321412\n"
     ]
    }
   ],
   "source": [
    "print(\"time taken to read :: \", (t2-t1)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i]['posts']=clean_my_string(data[i]['posts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i]['posts']=remove_stop_words(data[i]['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
