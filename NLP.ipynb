{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ebryx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ebryx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ebryx/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "# from xml.dom import minidom\n",
    "from lxml import etree\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to read All csv ::  0.12357783317565918  seconds \n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "path='./dataset/blogs_train/'\n",
    "all_files = glob.glob(path + \"/*.xml\")\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "  # print(\"file name \", filename)\n",
    "  li.append(filename)\n",
    "    \n",
    "\n",
    "t2=time.time()\n",
    "print(\"time taken to read All csv :: \", t2-t1, \" seconds \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file_names(li):\n",
    "  # names_list=[]\n",
    "  # for i in range(len(li)):\n",
    "  name_list=li.split('/')[-1]\n",
    "  return name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_attributes_from_name(name):\n",
    "  # data_object=[]\n",
    "  # for data in range(len(names_list)):\n",
    "  #   dummy_var=names_list[data].split('.')\n",
    "  #   dummy_object={'id':dummy_var[0], 'gender':dummy_var[1],\n",
    "  #                 'age':dummy_var[2], 'category':dummy_var[3],\n",
    "  #                 'horoscope':dummy_var[4]\n",
    "  #                 }\n",
    "  #   data_object.append(dummy_object)\n",
    "  dummy_var=name.split('.')\n",
    "  data_object={'id':dummy_var[0], 'gender':dummy_var[1],\n",
    "                  'age':dummy_var[2], 'category':dummy_var[3],\n",
    "                  'horoscope':dummy_var[4]\n",
    "                }\n",
    "  \n",
    "  return data_object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = etree.XMLParser(encoding='ISO-8859-1', recover = True, huge_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(li):\n",
    "    \n",
    "    \n",
    "    all_objects=[]\n",
    "    for i in range(len(li)):\n",
    "    # print(\"i will run \", i+1,\" times\")\n",
    "        name_of_file=clean_file_names(li[i])\n",
    "    # print(name_of_file)\n",
    "        data_object=get_attributes_from_name(name_of_file)\n",
    "    # print(data_object)\n",
    "\n",
    "        post_text='  '\n",
    "        tree = etree.parse(li[i], parser=parser)\n",
    "        root = tree.getroot()\n",
    "        for element in root.iter():\n",
    "            if element.tag == 'post':\n",
    "                element_text = element.text\n",
    "                post_text = post_text + element_text\n",
    "        mystring = post_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "        mystring = mystring.replace('   ', '').replace('\\\\','')\n",
    "   \n",
    "\n",
    "\n",
    "   \n",
    "    # print(mystring)\n",
    "    # name_of_file=clean_file_names(li[i])\n",
    "    # data_object=get_attributes_from_name(name_of_file)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        data_object['posts']=mystring   \n",
    "        all_objects.append(data_object)\n",
    "\n",
    "    return all_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "data=read_xml(li[0:])\n",
    "t2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def remove_stop_words(posts):\n",
    "    filtered_post=[]\n",
    "    \n",
    "    word_tokens = word_tokenize(posts) \n",
    "    filtered_sentence = []   \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    filtered_post.append(filtered_sentence)\n",
    "    return filtered_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_remove_stop_words(posts):\n",
    "    filtered=''\n",
    "    for x in posts.split(' '):\n",
    "        if x not in stop_words:\n",
    "            filtered+=' '+x\n",
    "    return filtered    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i]['cleaned_stop_words']=fun_remove_stop_words(data[i]['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dashes(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i]['cleaned_stop_words']=data[i]['cleaned_stop_words'].replace('--', ' ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dashes(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  love studying?can't get enough fat books manic profs.?  well, rare species, need worry, This dedicated uncle, died duty age 29, police work... The Day I Cried                  â\\x80\\x9cThose I fight I hate,Those I guard I love;â\\x80\\x9d  - (An Irish Airman Foresees His Death, W.B. Yeats)  It white room, The corpse winding sheet, My aunt new white sari, Staring silence gone. I could hear leaves rustle outside window, But I didnâ\\x80\\x99t cry.  She gone crazy, mom whispered, Between sobs, â\\x80\\x9cShe 25, poor girlâ\\x80\\x9d I continued look down, At mosaic floor, registering word. I could hear white saris softly flapping, But I couldnâ\\x80\\x99t cry.  My cousin sister came me, Dressed prettiest frock occasion. She five, understand? I took buy chocolate. I could hear neighborsâ\\x80\\x99 shushing went by, But I still didnâ\\x80\\x99t cry.  On way back train, Three passengers discussing police, â\\x80\\x9cAll corrupt sons bitches â\\x80\\x93 deserve dieâ\\x80\\x9d I remembered you, uncle, proud uniform. I could still hear aunt screaming nani lit pyre, Thatâ\\x80\\x99s I cried. V.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]['cleaned_stop_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['cleaned_stop_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= []\n",
    "for hor,gen in zip(df['horoscope'],df['gender']):\n",
    "    labels.append((hor,gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes  ::  14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Aquarius', 'Aries', 'Cancer', 'Capricorn', 'Gemini', 'Leo',\n",
       "       'Libra', 'Pisces', 'Sagittarius', 'Scorpio', 'Taurus', 'Virgo',\n",
       "       'female', 'male'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(labels)\n",
    "classes=len(mlb.classes_)\n",
    "print(\"classes  :: \", classes)\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = mlb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_dim = trainX.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(20, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(10,  activation='tanh'))\n",
    "# model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(classes, activation='softmax'))\n",
    "# model.add(Activation('softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 20)                16218700  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              11264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14)                14350     \n",
      "=================================================================\n",
      "Total params: 16,248,620\n",
      "Trainable params: 16,246,572\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 11680 samples, validate on 2920 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ebryx/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "11680/11680 [==============================] - 272s 23ms/step - loss: 0.3841 - acc: 0.8576 - val_loss: 0.4962 - val_acc: 0.8641\n",
      "Epoch 2/5\n",
      "11680/11680 [==============================] - 262s 22ms/step - loss: 0.3784 - acc: 0.8606 - val_loss: 0.4847 - val_acc: 0.8583\n",
      "Epoch 3/5\n",
      " 1350/11680 [==>...........................] - ETA: 3:27 - loss: 0.3741 - acc: 0.8669"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY\n",
    "classifier_nn = model.fit(trainX,trainY,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(testX, testY),\n",
    "                    batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(trainX[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time taken to read :: \", (t2-t1)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup=data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2 Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ca04f73db491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mwords_of_posts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "words_of_posts=[]\n",
    "for j in range(len(data)):\n",
    "    posts=data[j]['posts']\n",
    "    \n",
    "    words=[]\n",
    "    for i in range(len(posts)):\n",
    "        words.append(word_tokenize(posts[i]))\n",
    "    words_of_posts.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love',\n",
       "  'studying',\n",
       "  '?',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'get',\n",
       "  'enough',\n",
       "  'fat',\n",
       "  'books',\n",
       "  'manic',\n",
       "  'profs.',\n",
       "  '?',\n",
       "  'well',\n",
       "  ',',\n",
       "  'rare',\n",
       "  'species',\n",
       "  ',',\n",
       "  'need',\n",
       "  'worry',\n",
       "  ',',\n",
       "  'This',\n",
       "  'dedicated',\n",
       "  'uncle',\n",
       "  ',',\n",
       "  'died',\n",
       "  'duty',\n",
       "  'age',\n",
       "  '29',\n",
       "  ',',\n",
       "  'police',\n",
       "  'work',\n",
       "  '...',\n",
       "  'The',\n",
       "  'Day',\n",
       "  'I',\n",
       "  'Cried',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  'â\\x80\\x9cThose',\n",
       "  'I',\n",
       "  'fight',\n",
       "  'I',\n",
       "  'hate',\n",
       "  ',',\n",
       "  'Those',\n",
       "  'I',\n",
       "  'guard',\n",
       "  'I',\n",
       "  'love',\n",
       "  ';',\n",
       "  'â\\x80\\x9d',\n",
       "  '-',\n",
       "  '(',\n",
       "  'An',\n",
       "  'Irish',\n",
       "  'Airman',\n",
       "  'Foresees',\n",
       "  'His',\n",
       "  'Death',\n",
       "  ',',\n",
       "  'W.B',\n",
       "  '.',\n",
       "  'Yeats',\n",
       "  ')',\n",
       "  'It',\n",
       "  'white',\n",
       "  'room',\n",
       "  ',',\n",
       "  'The',\n",
       "  'corpse',\n",
       "  'winding',\n",
       "  'sheet',\n",
       "  ',',\n",
       "  'My',\n",
       "  'aunt',\n",
       "  'new',\n",
       "  'white',\n",
       "  'sari',\n",
       "  ',',\n",
       "  'Staring',\n",
       "  'silence',\n",
       "  'gone',\n",
       "  '.',\n",
       "  'I',\n",
       "  'could',\n",
       "  'hear',\n",
       "  'leaves',\n",
       "  'rustle',\n",
       "  'outside',\n",
       "  'window',\n",
       "  ',',\n",
       "  'But',\n",
       "  'I',\n",
       "  'didnâ\\x80\\x99t',\n",
       "  'cry',\n",
       "  '.',\n",
       "  'She',\n",
       "  'gone',\n",
       "  'crazy',\n",
       "  ',',\n",
       "  'mom',\n",
       "  'whispered',\n",
       "  ',',\n",
       "  'Between',\n",
       "  'sobs',\n",
       "  ',',\n",
       "  'â\\x80\\x9cShe',\n",
       "  '25',\n",
       "  ',',\n",
       "  'poor',\n",
       "  'girlâ\\x80\\x9d',\n",
       "  'I',\n",
       "  'continued',\n",
       "  'look',\n",
       "  ',',\n",
       "  'At',\n",
       "  'mosaic',\n",
       "  'floor',\n",
       "  ',',\n",
       "  'registering',\n",
       "  'word',\n",
       "  '.',\n",
       "  'I',\n",
       "  'could',\n",
       "  'hear',\n",
       "  'white',\n",
       "  'saris',\n",
       "  'softly',\n",
       "  'flapping',\n",
       "  ',',\n",
       "  'But',\n",
       "  'I',\n",
       "  'couldnâ\\x80\\x99t',\n",
       "  'cry',\n",
       "  '.',\n",
       "  'My',\n",
       "  'cousin',\n",
       "  'sister',\n",
       "  'came',\n",
       "  ',',\n",
       "  'Dressed',\n",
       "  'prettiest',\n",
       "  'frock',\n",
       "  'occasion',\n",
       "  '.',\n",
       "  'She',\n",
       "  'five',\n",
       "  ',',\n",
       "  'understand',\n",
       "  '?',\n",
       "  'I',\n",
       "  'took',\n",
       "  'buy',\n",
       "  'chocolate',\n",
       "  '.',\n",
       "  'I',\n",
       "  'could',\n",
       "  'hear',\n",
       "  'neighborsâ\\x80\\x99',\n",
       "  'shushing',\n",
       "  'went',\n",
       "  ',',\n",
       "  'But',\n",
       "  'I',\n",
       "  'still',\n",
       "  'didnâ\\x80\\x99t',\n",
       "  'cry',\n",
       "  '.',\n",
       "  'On',\n",
       "  'way',\n",
       "  'back',\n",
       "  'train',\n",
       "  ',',\n",
       "  'Three',\n",
       "  'passengers',\n",
       "  'discussing',\n",
       "  'police',\n",
       "  ',',\n",
       "  'â\\x80\\x9cAll',\n",
       "  'corrupt',\n",
       "  'sons',\n",
       "  'bitches',\n",
       "  'â\\x80\\x93',\n",
       "  'deserve',\n",
       "  'dieâ\\x80\\x9d',\n",
       "  'I',\n",
       "  'remembered',\n",
       "  ',',\n",
       "  'uncle',\n",
       "  ',',\n",
       "  'proud',\n",
       "  'uniform',\n",
       "  '.',\n",
       "  'I',\n",
       "  'could',\n",
       "  'still',\n",
       "  'hear',\n",
       "  'aunt',\n",
       "  'screaming',\n",
       "  'nani',\n",
       "  'lit',\n",
       "  'pyre',\n",
       "  ',',\n",
       "  'Thatâ\\x80\\x99s',\n",
       "  'I',\n",
       "  'cried.',\n",
       "  '--',\n",
       "  'V',\n",
       "  '.']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stop_words(data[1]['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data)==len(words_of_posts):\n",
    "    for i in range(len(data)):\n",
    "        data[i]['tokenized']=words_of_posts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['tokenized'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d98f9647d1af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \"\"\"\n\u001b[1;32m   1185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['horoscope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_transform=le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y = np_utils.to_categorical(y_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_matrix=_create_frequency_matrix(data[1]['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix=_create_tf_matrix(freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_per_word=_create_documents_per_words(freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' love studying?': {'love': 0.8129133566428556,\n",
       "  'studi': 1.1139433523068367,\n",
       "  '?': 0.8129133566428556,\n",
       "  'ca': 1.1139433523068367,\n",
       "  \"n't\": 1.1139433523068367,\n",
       "  'get': 1.1139433523068367,\n",
       "  'enough': 1.1139433523068367,\n",
       "  'fat': 1.1139433523068367,\n",
       "  'book': 1.1139433523068367,\n",
       "  'manic': 1.1139433523068367,\n",
       "  'prof': 1.1139433523068367,\n",
       "  '.': 0.15970084286751188},\n",
       " 'well, you are a': {'well': 1.1139433523068367,\n",
       "  ',': 0.15970084286751188,\n",
       "  'rare': 1.1139433523068367,\n",
       "  'speci': 1.1139433523068367,\n",
       "  'need': 1.1139433523068367,\n",
       "  'worri': 1.1139433523068367,\n",
       "  'thi': 1.1139433523068367,\n",
       "  'dedic': 1.1139433523068367,\n",
       "  'uncl': 0.8129133566428556,\n",
       "  'die': 1.1139433523068367,\n",
       "  'duti': 1.1139433523068367,\n",
       "  'age': 1.1139433523068367,\n",
       "  '29': 1.1139433523068367,\n",
       "  'polic': 0.8129133566428556,\n",
       "  'work': 1.1139433523068367,\n",
       "  '...': 1.1139433523068367},\n",
       " 'The Day I Cried': {'day': 1.1139433523068367,\n",
       "  'criedâ\\x80\\x9cthos': 1.1139433523068367,\n",
       "  'fight': 1.1139433523068367,\n",
       "  'hate': 1.1139433523068367,\n",
       "  ',': 0.15970084286751188,\n",
       "  'guard': 1.1139433523068367,\n",
       "  'love': 0.8129133566428556,\n",
       "  ';': 1.1139433523068367,\n",
       "  'â\\x80\\x9d': 1.1139433523068367,\n",
       "  '-': 1.1139433523068367,\n",
       "  '(': 1.1139433523068367,\n",
       "  'irish': 1.1139433523068367,\n",
       "  'airman': 1.1139433523068367,\n",
       "  'forese': 1.1139433523068367,\n",
       "  'hi': 0.8129133566428556,\n",
       "  'death': 1.1139433523068367,\n",
       "  'w.b': 1.1139433523068367,\n",
       "  '.': 0.15970084286751188},\n",
       " 'Yeats)  It was ': {'yeat': 1.1139433523068367,\n",
       "  ')': 1.1139433523068367,\n",
       "  'wa': 0.8129133566428556,\n",
       "  'white': 1.1139433523068367,\n",
       "  'room': 1.1139433523068367,\n",
       "  ',': 0.15970084286751188,\n",
       "  'corps': 1.1139433523068367,\n",
       "  'hi': 0.8129133566428556,\n",
       "  'wind': 1.1139433523068367,\n",
       "  'sheet': 1.1139433523068367,\n",
       "  'aunt': 0.8129133566428556,\n",
       "  'new': 1.1139433523068367,\n",
       "  'sari': 1.1139433523068367,\n",
       "  'stare': 1.1139433523068367,\n",
       "  'silenc': 1.1139433523068367,\n",
       "  'gone': 0.8129133566428556,\n",
       "  '.': 0.15970084286751188},\n",
       " 'I could hear th': {'could': 0.8129133566428556,\n",
       "  'hear': 0.8129133566428556,\n",
       "  'neighbor': 1.1139433523068367,\n",
       "  'shush': 1.1139433523068367,\n",
       "  'went': 1.1139433523068367,\n",
       "  ',': 0.15970084286751188,\n",
       "  'still': 0.8129133566428556,\n",
       "  'didnt': 1.1139433523068367,\n",
       "  'cri': 1.1139433523068367,\n",
       "  '.': 0.15970084286751188},\n",
       " 'She had gone cr': {'gone': 0.8129133566428556,\n",
       "  'crazi': 1.1139433523068367,\n",
       "  ',': 0.15970084286751188,\n",
       "  'mom': 1.1139433523068367,\n",
       "  'whisper': 1.1139433523068367,\n",
       "  'sob': 1.1139433523068367,\n",
       "  'â\\x80\\x9cshe': 1.1139433523068367,\n",
       "  '25': 1.1139433523068367,\n",
       "  'poor': 1.1139433523068367,\n",
       "  'girlâ\\x80\\x9d': 1.1139433523068367,\n",
       "  'continu': 1.1139433523068367,\n",
       "  'look': 1.1139433523068367,\n",
       "  'mosaic': 1.1139433523068367,\n",
       "  'floor': 1.1139433523068367,\n",
       "  'regist': 1.1139433523068367,\n",
       "  'word': 1.1139433523068367,\n",
       "  '.': 0.15970084286751188},\n",
       " 'My cousin siste': {'cousin': 1.1139433523068367,\n",
       "  'sister': 1.1139433523068367,\n",
       "  'came': 1.1139433523068367,\n",
       "  ',': 0.15970084286751188,\n",
       "  'dress': 1.1139433523068367,\n",
       "  'prettiest': 1.1139433523068367,\n",
       "  'frock': 1.1139433523068367,\n",
       "  'occas': 1.1139433523068367,\n",
       "  '.': 0.15970084286751188},\n",
       " 'She was five, d': {'wa': 0.8129133566428556,\n",
       "  'five': 1.1139433523068367,\n",
       "  ',': 0.15970084286751188,\n",
       "  'understand': 1.1139433523068367,\n",
       "  '?': 0.8129133566428556},\n",
       " 'I took her out ': {'took': 1.1139433523068367,\n",
       "  'buy': 1.1139433523068367,\n",
       "  'chocol': 1.1139433523068367,\n",
       "  '.': 0.15970084286751188},\n",
       " 'On my way back ': {'way': 1.1139433523068367,\n",
       "  'back': 1.1139433523068367,\n",
       "  'train': 1.1139433523068367,\n",
       "  ',': 0.15970084286751188,\n",
       "  'three': 1.1139433523068367,\n",
       "  'passeng': 1.1139433523068367,\n",
       "  'discuss': 1.1139433523068367,\n",
       "  'polic': 0.8129133566428556,\n",
       "  'â\\x80\\x9call': 1.1139433523068367,\n",
       "  'corrupt': 1.1139433523068367,\n",
       "  'son': 1.1139433523068367,\n",
       "  'bitch': 1.1139433523068367,\n",
       "  'â\\x80\\x93': 1.1139433523068367,\n",
       "  'deserv': 1.1139433523068367,\n",
       "  'dieâ\\x80\\x9d': 1.1139433523068367,\n",
       "  'rememb': 1.1139433523068367,\n",
       "  'uncl': 0.8129133566428556,\n",
       "  'proud': 1.1139433523068367,\n",
       "  'uniform': 1.1139433523068367,\n",
       "  '.': 0.15970084286751188},\n",
       " 'I could still h': {'could': 0.8129133566428556,\n",
       "  'still': 0.8129133566428556,\n",
       "  'hear': 0.8129133566428556,\n",
       "  'aunt': 0.8129133566428556,\n",
       "  'scream': 1.1139433523068367,\n",
       "  'nani': 1.1139433523068367,\n",
       "  'lit': 1.1139433523068367,\n",
       "  'pyre': 1.1139433523068367,\n",
       "  ',': 0.15970084286751188,\n",
       "  'cried.v': 1.1139433523068367,\n",
       "  '.': 0.15970084286751188}}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_create_idf_matrix(freq_matrix,doc_per_word,total_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
